{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c50b1c-819b-41d8-9984-21d15ebb40a5",
   "metadata": {},
   "source": [
    "# Feature Assignment\n",
    "\n",
    "Q.1)What is a parameter?\n",
    "Ans:- A parameter in Machine Learning (and statistics) is a value inside a model that is learned from data during training. Think of it as the \"knobs\" the algorithm adjusts to best fit the data.\n",
    "Parameters are internal to the model and are optimized during training. They differ from hyperparameters, which are set by the practitioner before training (e.g., learning rate, number of trees in a random forest). The goal of training is to find parameter values that minimize the loss function (error between predictions and actual outcomes).\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.2)What is correlation? What does negative correlation mean?\n",
    "Ans:-Correlation is a statistical measure that describes how strongly two variables are related to each other.It tells us whether an increase (or decrease) in one variable is associated with an increase (or decrease) in another.\n",
    "The correlation coefficient (often denoted as r) ranges between -1 and +1:\n",
    "+1 ‚Üí perfect positive correlation (variables move together).\n",
    "0 ‚Üí no correlation (no linear relationship).\n",
    "-1 ‚Üí perfect negative correlation (variables move in opposite directions).\n",
    "\n",
    "Negative Correlation = A negative correlation means that as one variable increases, the other decreases.\n",
    "Example:\n",
    "- The number of hours spent exercising vs. body fat percentage ‚Üí more exercise, lower body fat.\n",
    "- Price of a product vs. demand ‚Üí higher price, lower demand.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.3)Define Machine Learning. What are the main components in Machine Learning?\n",
    "Ans:-Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that enables computers to learn patterns from data and make predictions or decisions without being explicitly programmed. Instead of following fixed rules, ML models improve their performance by learning from examples and adjusting internal parameters to minimize errors. It powers applications like image recognition, fraud detection, recommendation systems, and natural language processing.\n",
    "\n",
    "Main Components of Machine Learning: Machine Learning systems typically involve the following key components:\n",
    "\n",
    "1) Data\n",
    "- Raw information collected for training and testing.\n",
    "- Can be structured (tables) or unstructured (images, text).\n",
    "2) Features\n",
    "- Relevant attributes or variables extracted from data.\n",
    "- Example: In predicting house prices, features could be size, location, and number of rooms.\n",
    "3) Model\n",
    "- The mathematical or algorithmic structure that maps inputs (features) to outputs (predictions).\n",
    "- Examples: Linear regression, decision trees, neural networks.\n",
    "4) Training Process\n",
    "- The phase where the model learns by adjusting parameters using training data.\n",
    "- Involves minimizing a loss function (error measure).\n",
    "5) Evaluation\n",
    "- Testing the model on unseen data (test set) to measure performance.\n",
    "- Metrics include accuracy, precision, recall, F1-score, etc.\n",
    "6) Optimization\n",
    "- Algorithms (optimizers) like Gradient Descent are used to update parameters efficiently.\n",
    "7) Deployment\n",
    "- Once validated, the model is integrated into real-world applications for predictions or automation.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.4) How does loss value help in determining whether the model is good or not?\n",
    "Ans:-The loss value is a numerical measure of how far off a model‚Äôs predictions are from the actual outcomes. It‚Äôs central to determining whether a model is ‚Äúgood‚Äù or not.\n",
    "\n",
    "Loss is the output of a loss function, which compares predicted values to true labels.\n",
    "#Interpretation:\n",
    "Low loss ‚Üí Predictions are close to actual values ‚Üí Model is performing well.\n",
    "High loss ‚Üí Predictions are far from actual values ‚Üí Model is performing poorly.\n",
    "#Training Goal: During training, optimizers (like Gradient Descent) adjust model parameters to minimize loss.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.5)What are continuous and categorical variables?\n",
    "Ans:-In Machine Learning and statistics, variables are broadly classified into continuous and categorical types.\n",
    "\n",
    "1) Continous variables:- Represent numeric values that can take on an infinite number of possible values within a range.\n",
    "Examples:\n",
    "- Height (e.g., 170.5 cm)\n",
    "- Weight (e.g., 65.2 kg)\n",
    "- Temperature (e.g., 36.7 ¬∞C)\n",
    "They are measured on a scale and often used in regression problems.\n",
    "\n",
    "2) categorical Variables:-Represent discrete labels or categories rather than numeric ranges.\n",
    "Examples:\n",
    "- Gender (Male, Female, Other)\n",
    "- Color (Red, Blue, Green)\n",
    "- Country (India, USA, Japan)\n",
    "They are often encoded into numbers before being used in ML models.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.6)How do we handle categorical variables in Machine Learning? What are the common t\n",
    "echniques?\n",
    "Ans:-Handling categorical variables is a crucial step in machine learning because most algorithms require numerical input. If categorical data isn‚Äôt properly processed, models can misinterpret relationships and produce poor results. Here are the most common techniques:\n",
    "\n",
    "Common Techniques for Handling Categorical Variables:-\n",
    "1. Label Encoding\n",
    "- Assigns each category a unique integer.\n",
    "- Example: Red=0, Blue=1, Green=2.\n",
    "- Best for: Ordinal data (where categories have a natural order).\n",
    "- Risk: Can mislead models into thinking categories have numerical relationships when they don‚Äôt.\n",
    "\n",
    "2. One-Hot Encoding\n",
    "- Creates binary columns for each category.\n",
    "- Example: Color=Red ‚Üí [1,0,0], Color=Blue ‚Üí [0,1,0].\n",
    "- Best for: Nominal data (no inherent order).\n",
    "- Risk: High dimensionality if categories are numerous.\n",
    "\n",
    "3. Binary Encoding\n",
    "- Converts categories into binary numbers and splits them into separate columns.\n",
    "- Example: Category 5 ‚Üí Binary 101 ‚Üí [1,0,1].\n",
    "- Best for: High-cardinality features (many unique categories).\n",
    "- Benefit: Reduces dimensionality compared to one-hot encoding.\n",
    "\n",
    "4. Target / Mean Encoding\n",
    "- Replaces categories with the mean of the target variable for that category.\n",
    "- Example: If City=A has average sales of 200, encode City=A as 200.\n",
    "- Best for: Tree-based models.\n",
    "- Risk: Can cause data leakage if not applied carefully (must use cross-validation).\n",
    "\n",
    "Choosing the Right Technique\n",
    "- Nominal data (no order): One-hot, binary, or embeddings.\n",
    "- Ordinal data (ordered): Label encoding or ordinal mapping.\n",
    "- High-cardinality features: Binary, target, or embeddings.\n",
    "- Tree-based models: Target/frequency encoding often works well.\n",
    "- Linear models: One-hot encoding is usually safer.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.7) What do you mean by training and testing a dataset?\n",
    "Ans:-In machine learning, training and testing a dataset are two fundamental steps to ensure that a model learns patterns correctly and can generalize to new, unseen data.\n",
    "\n",
    "Training Dataset\n",
    "- This is the portion of the data used to teach the model.\n",
    "- The algorithm looks at the input features and the corresponding output (labels) to learn relationships.\n",
    "- Example: If you‚Äôre predicting house prices, the training set contains house features (size, location, etc.) and their actual prices.\n",
    "- The model adjusts its internal parameters during training to minimize errors.\n",
    "\n",
    "Testing Dataset\n",
    "- This is a separate portion of the data used to evaluate the model‚Äôs performance.\n",
    "- The model makes predictions on this unseen data, and we compare them to the actual outcomes.\n",
    "- Example: Using new house data (not seen during training) to check if the model predicts prices accurately.\n",
    "- Helps detect overfitting (when a model memorizes training data but fails on new data).\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.8)What is sklearn.preprocessing?\n",
    "Ans:- sklearn.preprocessing is a module in scikit-learn that provides a wide range of tools to prepare and transform data before feeding it into machine learning models. Preprocessing is essential because raw data often contains categorical variables, missing values, or features on different scales, which can negatively affect model performance.\n",
    "\n",
    "Key Functions in sklearn.preprocessing:-\n",
    "1. Scaling and Normalization\n",
    "- StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
    "- MinMaxScaler: Scales features to a fixed range (usually 0‚Äì1).\n",
    "- RobustScaler: Uses median and interquartile range, robust to outliers.\n",
    "- Normalizer: Normalizes samples individually to unit norm (useful for text or sparse data).\n",
    "\n",
    "2. Encoding Categorical Variables\n",
    "- LabelEncoder: Converts categorical labels into integers.\n",
    "- OneHotEncoder: Converts categorical features into one-hot (binary) vectors.\n",
    "- OrdinalEncoder: Encodes categories with an integer value, preserving order.\n",
    "\n",
    "3. Feature Transformation\n",
    "- PolynomialFeatures: Generates polynomial and interaction features.\n",
    "- PowerTransformer: Applies power transformations (Box-Cox, Yeo-Johnson) to stabilize variance.\n",
    "- QuantileTransformer: Maps data to a uniform or normal distribution.\n",
    "\n",
    "4. Binarization\n",
    "- Binarizer: Converts numerical values into binary (0/1) based on a threshold.\n",
    "\n",
    "5. Imputation\n",
    "- While not strictly in preprocessing (it‚Äôs in sklearn.impute), imputation is often used together:\n",
    "SimpleImputer: Fills missing values with mean, median, or most frequent value.\n",
    "KNNImputer: Uses nearest neighbors to fill missing values.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.9)What is a Test set?\n",
    "Ans:-A test set is the portion of your dataset that you keep aside to evaluate how well your machine learning model performs on unseen data.\n",
    "It checks whether the model can generalize beyond the data it was trained on. Think of it as the \"exam\" after the \"study\" phase.The test set should be representative of the overall dataset but must not overlap with the training data. This ensures the model isn‚Äôt just memorizing.\n",
    "Usage:- \n",
    "- During training, the model never sees the test set.\n",
    "- After training is complete, you run the model on the test set to measure performance metrics like accuracy, precision, recall, F1-score, or mean squared error (depending on the task).\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.10)How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
    "Ans:-In Python, the most common way to split data into training and testing sets is by using scikit-learn‚Äôs train_test_split function. This ensures your model learns from one portion of the data and is evaluated on another portion it hasn‚Äôt seen before.\n",
    "\n",
    "Approaching a machine learning problem is a structured process‚Äîit‚Äôs not just about throwing data into an algorithm. You need to carefully define the problem, prepare the data, choose the right model, and evaluate results.\n",
    "\n",
    "Steps to Approach a Machine Learning Problem\n",
    "1. Define the Problem\n",
    "- Understand the business or research question.\n",
    "- Decide whether it‚Äôs classification, regression, clustering, or recommendation.\n",
    "- Identify the target variable (what you want to predict).\n",
    "\n",
    "2. Collect and Explore Data\n",
    "- Gather relevant datasets.\n",
    "- Perform Exploratory Data Analysis (EDA):\n",
    "- Check distributions, correlations, missing values, and outliers.\n",
    "- Visualize relationships between features and target.\n",
    "\n",
    "3. Preprocess the Data\n",
    "- Handle missing values (imputation).\n",
    "- Encode categorical variables (one-hot, label, target encoding).\n",
    "- Scale/normalize numerical features (StandardScaler, MinMaxScaler).\n",
    "- Feature engineering (create new features, polynomial terms, embeddings).\n",
    "\n",
    "4. Split Data\n",
    "- Divide into training and testing sets (e.g., 80/20).\n",
    "- Optionally create a validation set or use cross-validation for robust evaluation.\n",
    "\n",
    "5. Select and Train a Model\n",
    "- Choose algorithms based on problem type:\n",
    "- Classification ‚Üí Logistic Regression, Decision Trees, Random Forests, SVM, Neural Networks.\n",
    "- Regression ‚Üí Linear Regression, Gradient Boosting, Random Forests, Neural Networks.\n",
    "- Clustering ‚Üí K-Means, DBSCAN, Hierarchical Clustering.\n",
    "- Train the model on the training set.\n",
    "\n",
    "6. Evaluate the Model\n",
    "- Use appropriate metrics:\n",
    "- Classification ‚Üí Accuracy, Precision, Recall, F1-score, ROC-AUC.\n",
    "- Regression ‚Üí RMSE, MAE, R¬≤.\n",
    "- Compare performance across models.\n",
    "\n",
    "7. Tune Hyperparameters\n",
    "- Use Grid Search or Random Search with cross-validation.\n",
    "- Optimize parameters for better performance without overfitting.\n",
    "\n",
    "8. Deploy and Monitor\n",
    "- Deploy the model into production (API, app, or pipeline).\n",
    "- Continuously monitor performance on new data.\n",
    "- Retrain periodically as data evolves (concept drift).\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.11)Why do we have to perform EDA before fitting a model to the data?\n",
    "Ans:- Performing Exploratory Data Analysis (EDA) before fitting a machine learning model is essential because it helps you truly understand your dataset and avoid costly mistakes later. Skipping EDA is like trying to solve a puzzle without first looking at the pieces.\n",
    "\n",
    "EDA is Important:-\n",
    "1. Understand Data Distribution\n",
    "- Identify how features are spread (normal, skewed, uniform).\n",
    "- Spot imbalances in target classes (important for classification).\n",
    "\n",
    "2. Detect Missing Values & Outliers\n",
    "- Missing data can bias models if not handled.\n",
    "- Outliers can distort training, especially in regression tasks.\n",
    "\n",
    "3. Reveal Relationships Between Variables\n",
    "- Correlation analysis shows which features are strongly related to the target.\n",
    "- Helps in feature selection and engineering.\n",
    "\n",
    "4. Spot Data Quality Issues\n",
    "- Incorrect data types (e.g., numbers stored as strings).\n",
    "- Duplicate records or inconsistent labels.\n",
    "\n",
    "5. Guide Feature Engineering\n",
    "- EDA highlights opportunities to create new features (ratios, interactions).\n",
    "- Helps decide whether scaling, encoding, or transformations are needed.\n",
    "\n",
    "6. Prevent Overfitting & Bias\n",
    "- By understanding distributions and relationships, you avoid blindly fitting a model that memorizes quirks instead of learning patterns.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.12)What is correlation?\n",
    "Ans:-Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. In simple terms, it tells us whether‚Äîand how strongly‚Äîchanges in one variable are associated with changes in another.Correlation quantifies the degree to which two variables move together. If one increases when the other increases, they have a positive correlation; if one decreases when the other increases, they have a negative correlation.The correlation coefficient (often Pearson‚Äôs ùëü) ranges from -1 to +1.\n",
    "\n",
    "Types:\n",
    "- Positive correlation: Height and weight (taller people tend to weigh more).\n",
    "- Negative correlation: Price and demand (higher prices often reduce demand).\n",
    "- Zero correlation: Shoe size and intelligence (no relationship).\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.13)What does negative correlation mean?\n",
    "Ans:-A negative correlation means that as one variable increases, the other tends to decrease. In other words, the two variables move in opposite directions.\n",
    "- Correlation coefficient (r): Negative values (between 0 and -1) indicate negative correlation.\n",
    "- Strength:\n",
    "  ùëü=‚àí1: Perfect negative correlation (one goes up, the other always goes down proportionally).\n",
    "  ùëü=0: No correlation.\n",
    "  Values closer to -1 show stronger negative relationships.\n",
    "\n",
    "Examples\n",
    "- Price vs. Demand: As the price of a product increases, demand usually decreases.\n",
    "- Exercise vs. Weight: More exercise often leads to lower body weight.\n",
    "- Speed vs. Travel Time: Higher speed reduces travel time.\n",
    "\n",
    "Negative correlation does not imply causation. Just because two variables move in opposite directions doesn‚Äôt mean one directly causes the other‚Äîit could be influenced by other factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a637fa12-ec7e-44dc-ae11-ac8c981fe84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Q.14)How can you find correlation between variables in Python?\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = [2, 4, 6, 8, 10]\n",
    "y = [50, 60, 70, 80, 90]\n",
    "\n",
    "correlation = np.corrcoef(x, y)[0, 1]\n",
    "print(\"Correlation:\", correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769b065-ec25-499a-b62c-fccff9fcc779",
   "metadata": {},
   "source": [
    "Q.15)What is causation? Explain difference between correlation and causation with an example.\n",
    "Ans:- Causation means that one event or variable directly influences another. In other words, changes in one variable are the reason for changes in another.\n",
    "\n",
    "Difference Between Correlation and Causation:-\n",
    "1. Correlation\n",
    "- Measures the strength and direction of a relationship between two variables.\n",
    "- Does not prove that one variable causes the other.\n",
    "- Example: Ice cream sales and drowning incidents are positively correlated (both increase in summer), but eating ice cream does not cause drowning. The hidden factor is hot weather.\n",
    "\n",
    "2. Causation\n",
    "- Implies a cause-and-effect relationship.\n",
    "- One variable directly impacts the other.\n",
    "- Example: Increasing the amount of fertilizer applied to crops (cause) leads to higher crop yields (effect).\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.16) What is an Optimizer? What are different types of optimizers? Explain each with an example\n",
    "Ans:- An optimizer in machine learning (especially deep learning) is an algorithm that adjusts the parameters (weights and biases) of a model to minimize the loss function. In simple terms, it‚Äôs the method that guides the model on how to learn from data by improving predictions step by step.\n",
    "\n",
    "Common Types of Optimizers\n",
    "1. Gradient Descent (GD)\n",
    "- How it works: Updates parameters by moving in the opposite direction of the gradient of the loss function.\n",
    "- Variants:\n",
    "- Batch Gradient Descent: Uses the entire dataset for each update.\n",
    "- Stochastic Gradient Descent (SGD): Uses one sample at a time (faster, but noisier).\n",
    "- Mini-Batch Gradient Descent: Uses small batches (balances speed and stability).\n",
    "- Example: Training a linear regression model with SGD to minimize mean squared error.\n",
    "\n",
    "2. Momentum\n",
    "- How it works: Adds a fraction of the previous update to the current one, helping accelerate learning and avoid local minima.\n",
    "- Example: In image classification, momentum helps the model converge faster by smoothing oscillations.\n",
    "\n",
    "3. Nesterov Accelerated Gradient (NAG)\n",
    "- How it works: Similar to momentum but looks ahead at the future position before updating, leading to more accurate steps.\n",
    "- Example: Used in deep networks to improve convergence speed compared to plain momentum.\n",
    "\n",
    "4. Adagrad\n",
    "- How it works: Adapts the learning rate for each parameter based on past gradients. Parameters with frequent updates get smaller learning rates.\n",
    "- Example: Useful in sparse data problems like natural language processing (NLP).\n",
    "\n",
    "5. RMSProp\n",
    "- How it works: Maintains a moving average of squared gradients to normalize updates, preventing the learning rate from shrinking too much.\n",
    "- Example: Commonly used in recurrent neural networks (RNNs) for sequence tasks.\n",
    "\n",
    "6. Adam (Adaptive Moment Estimation)\n",
    "- How it works: Combines momentum and RMSProp by keeping track of both first (mean) and second (variance) moments of gradients.\n",
    "- Example: Widely used in deep learning tasks like image recognition and NLP because it‚Äôs efficient and requires little tuning.\n",
    "\n",
    "7. AdaMax & Nadam\n",
    "- AdaMax: A variant of Adam using infinity norm, often more stable.\n",
    "- Nadam: Adam + Nesterov momentum, giving faster convergence in some cases.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.17)What is sklearn.linear_model?\n",
    "Ans:- sklearn.linear_model is a module in scikit-learn that provides implementations of various linear models for both regression and classification tasks. Linear models are based on the idea of modeling the relationship between input features and the target variable using a linear equation.\n",
    "sklearn.linear_model is our toolbox for linear regression and classification models, ranging from simple least squares to regularized and robust methods.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.18)What does model.fit() do? What arguments must be given?\n",
    "Ans:- In scikit-learn, the method model.fit() is the core function used to train a machine learning model. It takes in your training data (features and labels) and adjusts the model‚Äôs parameters so that it learns the relationship between inputs and outputs.\n",
    "- Learns patterns from the training data.\n",
    "- Updates internal parameters (like weights in regression or coefficients in logistic regression).\n",
    "- Prepares the model to make predictions on unseen data.\n",
    "\n",
    "Required Arguments\n",
    "- The arguments depend on the type of model (regression, classification, clustering, etc.), but generally:\n",
    "- X (Features/Input Data)\n",
    "  A 2D array or DataFrame of shape (n_samples, n_features).\n",
    "  Example: Age, income, education level.\n",
    "- y (Target/Labels)\n",
    "  A 1D array or Series of shape (n_samples,).\n",
    "  Example: House prices (for regression) or class labels (for classification).\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.19)What does model.predict() do? What arguments must be given?\n",
    "Ans:-In scikit-learn, model.predict() is the method used to generate predictions from a trained model. Once you‚Äôve fitted the model with model.fit(X, y), you call predict() to see how well the model performs on new or unseen data.\n",
    "\n",
    "model.predict() Does:-\n",
    "- Takes input features (X) and applies the learned parameters (from training).\n",
    "- Produces predicted values:\n",
    " For regression models ‚Üí continuous values (e.g., house price = 250,000).\n",
    " For classification models ‚Üí class labels (e.g., spam vs. not spam).\n",
    "\n",
    "Required Arguments:-\n",
    "- X (Features/Input Data)\n",
    "   - A 2D array, list, or DataFrame of shape (n_samples, n_features).\n",
    "   - Must have the same number of features as the training data.\n",
    "   - Example: Age, income, education level for predicting salary.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.20)What are continuous and categorical variables?\n",
    "Ans:-In statistics and machine learning, variables are often classified into continuous and categorical types. Understanding the difference is crucial because it determines how you preprocess data and which models or encoding techniques you use.\n",
    "\n",
    "1. Continuous Variables\n",
    "- Variables that can take on an infinite number of values within a range.\n",
    "- They are Numeric, measurable, often real-valued in nature.\n",
    "- Examples:\n",
    "Height (e.g., 170.5 cm)\n",
    "Weight (e.g., 65.2 kg)\n",
    "Temperature (e.g., 36.7 ¬∞C)\n",
    "Income (e.g., ‚Çπ50,000 per month)\n",
    "- Use in ML: Often scaled or normalized before training. Regression models typically predict continuous variables.\n",
    "\n",
    "2. Categorical Variables\n",
    "- Variables that represent distinct groups or categories.\n",
    "- They are Qualitative, not inherently numeric in nature.\n",
    "- Types:\n",
    "   - Nominal: Categories with no natural order (e.g., colors: Red, Blue, Green).\n",
    "   - Ordinal: Categories with a meaningful order (e.g., education level: High School < Bachelor < Master < PhD).\n",
    "- Examples:\n",
    "Gender (Male, Female, Other)\n",
    "City (Pune, Delhi, Mumbai)\n",
    "Product type (Electronics, Clothing, Furniture)\n",
    "- Use in ML: Must be encoded (e.g., one-hot encoding, label encoding) before being used in most algorithms.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.21)What is feature scaling? How does it help in Machine Learning?\n",
    "Ans:-Feature scaling is the process of transforming your data so that all features (variables) are on a similar scale. In machine learning, this is important because many algorithms are sensitive to the magnitude of feature values. If one feature has values in the thousands and another in decimals, the model may give undue importance to the larger-scaled feature simply because of its range, not because it‚Äôs more predictive.\n",
    "\n",
    "Feature Scaling Helps:-\n",
    "- Improves convergence speed: Algorithms like gradient descent converge faster when features are scaled.\n",
    "- Prevents bias toward large values: Distance-based models (e.g., KNN, SVM, clustering) rely on Euclidean distance, which can be distorted if features are on different scales.\n",
    "- Stabilizes optimization: Regularization techniques (like Ridge/Lasso) penalize coefficients more fairly when features are scaled.\n",
    "- Better visualization: Scaled features make plots and comparisons more interpretable.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.22)How do we perform scaling in Python?\n",
    "Ans:-In Python, you can perform feature scaling using the scikit-learn preprocessing module. The most common approaches are Standardization and Normalization, and scikit-learn provides ready-to-use classes for both. Scaling ensures all features contribute fairly to the model. You just choose the right scaler, call .fit_transform() on your dataset, and you‚Äôre ready to train.\n",
    "\n",
    "Methods for Scaling in Python\n",
    "1. Standardization (Z-score scaling) - Transforms features to have mean = 0 and standard deviation = 1.\n",
    "2. Min-Max Normalization- Scales values to a fixed range, usually [0, 1].\n",
    "3. Robust Scaling - Uses median and interquartile range (IQR), making it less sensitive to outliers.\n",
    "4. MaxAbs Scaling - Scales by the maximum absolute value, keeping values in [-1, 1].\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.23)What is sklearn.preprocessing?\n",
    "Ans:-sklearn.preprocessing is a module in scikit-learn that provides tools for preprocessing and transforming data before feeding it into machine learning models. Preprocessing is essential because raw data often contains categorical variables, missing values, or features on different scales, which can negatively affect model performance.\n",
    "\n",
    "Key Functions in sklearn.preprocessing:-\n",
    "1. Scaling and Normalization\n",
    "- StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
    "- MinMaxScaler: Scales features to a fixed range (usually 0‚Äì1).\n",
    "- RobustScaler: Uses median and interquartile range, robust to outliers.\n",
    "- Normalizer: Normalizes samples individually to unit norm (useful for text or sparse data).\n",
    "\n",
    "2. Encoding Categorical Variables\n",
    "- LabelEncoder: Converts categorical labels into integers.\n",
    "- OneHotEncoder: Converts categorical features into one-hot (binary) vectors.\n",
    "- OrdinalEncoder: Encodes categories with an integer value, preserving order.\n",
    "\n",
    "3. Feature Transformation\n",
    "- PolynomialFeatures: Generates polynomial and interaction features.\n",
    "- PowerTransformer: Applies power transformations (Box-Cox, Yeo-Johnson) to stabilize variance.\n",
    "- QuantileTransformer: Maps data to a uniform or normal distribution.\n",
    "\n",
    "4. Binarization\n",
    "- Binarizer: Converts numerical values into binary (0/1) based on a threshold.\n",
    "\n",
    "5. Imputation\n",
    "- While not strictly in preprocessing (it‚Äôs in sklearn.impute), imputation is often used together:\n",
    "SimpleImputer: Fills missing values with mean, median, or most frequent value.\n",
    "KNNImputer: Uses nearest neighbors to fill missing values.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.24)How do we split data for model fitting (training and testing) in Python?\n",
    "Ans:- In Python, the most common way to split data into training and testing sets is by using scikit-learn‚Äôs train_test_split function. This ensures your model learns from one portion of the data and is evaluated on another portion it hasn‚Äôt seen before.\n",
    "\n",
    "Key Parameters\n",
    "- test_size ‚Üí Fraction or number of samples for testing (e.g., 0.2 = 20%).\n",
    "- train_size ‚Üí Can also be specified, but usually test_size is enough.\n",
    "- random_state ‚Üí Fixes the random seed so results are reproducible.\n",
    "- stratify ‚Üí Ensures class proportions are preserved (important for classification tasks).\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q.25)Explain data encoding?\n",
    "Ans:-Data encoding is the process of converting categorical (non-numeric) variables into a numerical format so that machine learning algorithms can use them effectively. Most ML models work with numbers, not text labels, so encoding is a crucial preprocessing step.\n",
    "\n",
    "Types of Data Encoding:-\n",
    "1. Label Encoding - Converts categories into integers.\n",
    "2. One-Hot Encoding - Creates binary columns for each category.\n",
    "3. Ordinal Encoding - Assigns integers based on order.\n",
    "4. Ordinal Encoding - Assigns integers based on order.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
